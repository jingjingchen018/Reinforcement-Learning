the RL working draft: https://github.com/jingjingchen018/Reinforcement-Learning/blob/master/rl_monograph_AJK.pdf
The second chapter of this RL working draft aims to characterize the optimal minimax sample complexity of estimating the optimal state-action value $Q^{\star}$ with a generative model $\hat{P}$. To get a better understanding about the content of this chapter, I read three articles about it. 

## Papers I have read

Du S S, Kakade S M, Wang R, et al. Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?[J]. arXiv preprint arXiv:1910.03016, 2019.

#### the major contributions
from this article, I know the motivation of the 2nd Chapter of the working draft of RL. There is an useful link about this article https://www.youtube.com/watch?v=i63WoK852q0
#### Comments
I read the abstract and the introduction of this paper, meanwhile, watched the video. 


the main content of Chapter2 comes from the work of Azar et al.https://github.com/jingjingchen018/Reinforcement-Learning/blob/master/Azar2013_Article_MinimaxPACBoundsOnTheSampleCom.pdf

#### the major contributions
The main objective of this paper is to estimate the bound of the estimating error.
#### Comments
I have proved half of this article.
###  

#### the major contributions

#### Comments

### 

#### the major contributions

#### Comments

