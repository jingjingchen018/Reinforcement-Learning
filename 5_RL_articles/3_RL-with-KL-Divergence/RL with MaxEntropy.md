# RL with MaxEntropy

## (0) What: MaxEntropy



## (1) Why: using MaxEntropy in RL

*（2020ICLR）If MaxEnt RL is the Answer, What is the Question*

出自卡耐基梅隆, UC 伯克利 google brain, 

> @article{eysenbach2019if,
>   title={If MaxEnt RL is the Answer, What is the Question?},
>   author={Eysenbach, Benjamin and Levine, Sergey},
>   journal={arXiv preprint arXiv:1910.01913},
>   year={2019}
> }







## (2) How: Three Scenes

### (2.1) As a regularization

 ####  (a) TRPO



#### (b) PPO

**Commonality**



### (2.2) As objective,

#### (a) Soft AC

#### (b) Soft Q-learning

**Commonality**



### (2.3) Using in MARL

#### (a) Wang Jun and Wen Ying et.al----applying MaxEntropy to MARL 

*P1 （ICJAI2019）A Regularized Opponent Model with Maximum Entropy Objective*

> @article{tian2019regularized,
>   title={A regularized opponent model with maximum entropy objective},
>   author={Tian, Zheng and Wen, Ying and Gong, Zhichen and Punakkath, Faiz and Zou, Shihao and Wang, Jun},
>   journal={arXiv preprint arXiv:1905.08087},
>   year={2019}
> }

*P2 Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning*

